Index: cmd/kube-controller-manager/app/policy.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cmd/kube-controller-manager/app/policy.go b/cmd/kube-controller-manager/app/policy.go
--- a/cmd/kube-controller-manager/app/policy.go	(revision 151a50e8e9872415562d9dc7b94081ef32fbef5f)
+++ b/cmd/kube-controller-manager/app/policy.go	(revision 1d028b69c6385bc4bcd5a678aec3186c3fae7da4)
@@ -53,6 +53,7 @@
 		ctx.InformerFactory.Apps().V1().ReplicaSets(),
 		ctx.InformerFactory.Apps().V1().Deployments(),
 		ctx.InformerFactory.Apps().V1().StatefulSets(),
+		ctx.InformerFactory.Apps().V1().DaemonSets(),
 		client,
 		ctx.RESTMapper,
 		scaleClient,
Index: pkg/controller/disruption/disruption.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pkg/controller/disruption/disruption.go b/pkg/controller/disruption/disruption.go
--- a/pkg/controller/disruption/disruption.go	(revision 151a50e8e9872415562d9dc7b94081ef32fbef5f)
+++ b/pkg/controller/disruption/disruption.go	(revision 1d028b69c6385bc4bcd5a678aec3186c3fae7da4)
@@ -95,6 +95,9 @@
 	ssLister       appsv1listers.StatefulSetLister
 	ssListerSynced cache.InformerSynced

+	dsLister       appsv1listers.DaemonSetLister
+	dsListerSynced cache.InformerSynced
+
 	// PodDisruptionBudget keys that need to be synced.
 	queue        workqueue.RateLimitingInterface
 	recheckQueue workqueue.DelayingInterface
@@ -123,6 +126,7 @@
 	rsInformer appsv1informers.ReplicaSetInformer,
 	dInformer appsv1informers.DeploymentInformer,
 	ssInformer appsv1informers.StatefulSetInformer,
+	dsInformer appsv1informers.DaemonSetInformer,
 	kubeClient clientset.Interface,
 	restMapper apimeta.RESTMapper,
 	scaleNamespacer scaleclient.ScalesGetter,
@@ -168,6 +172,9 @@
 	dc.ssLister = ssInformer.Lister()
 	dc.ssListerSynced = ssInformer.Informer().HasSynced

+	dc.dsLister = dsInformer.Lister()
+	dc.dsListerSynced = dsInformer.Informer().HasSynced
+
 	dc.mapper = restMapper
 	dc.scaleNamespacer = scaleNamespacer
 	dc.discoveryClient = discoveryClient
@@ -181,7 +188,7 @@
 // resources directly and only fall back to the scale subresource when needed.
 func (dc *DisruptionController) finders() []podControllerFinder {
 	return []podControllerFinder{dc.getPodReplicationController, dc.getPodDeployment, dc.getPodReplicaSet,
-		dc.getPodStatefulSet, dc.getScaleController}
+		dc.getPodStatefulSet, dc.getPodDaemonSet, dc.getScaleController}
 }

 var (
@@ -189,6 +196,7 @@
 	controllerKindSS  = apps.SchemeGroupVersion.WithKind("StatefulSet")
 	controllerKindRC  = v1.SchemeGroupVersion.WithKind("ReplicationController")
 	controllerKindDep = v1beta1.SchemeGroupVersion.WithKind("Deployment")
+	controllerKindDS  = apps.SchemeGroupVersion.WithKind("DaemonSet")
 )

 // getPodReplicaSet finds a replicaset which has no matching deployments.
@@ -265,6 +273,24 @@
 	return &controllerAndScale{deployment.UID, *(deployment.Spec.Replicas)}, nil
 }

+// getPodDaemonSet returns the daemonset referenced by the provided controllerRef.
+func (dc *DisruptionController) getPodDaemonSet(controllerRef *metav1.OwnerReference, namespace string) (*controllerAndScale, error) {
+	ok, err := verifyGroupKind(controllerRef, controllerKindDS.Kind, []string{"apps"})
+	if !ok || err != nil {
+		return nil, err
+	}
+	ds, err := dc.dsLister.DaemonSets(namespace).Get(controllerRef.Name)
+	if err != nil {
+		// The only possible error is NotFound, which is ok here.
+		return nil, nil
+	}
+	if ds.UID != controllerRef.UID {
+		return nil, nil
+	}
+
+	return &controllerAndScale{ds.UID, ds.Status.DesiredNumberScheduled}, nil
+}
+
 func (dc *DisruptionController) getPodReplicationController(controllerRef *metav1.OwnerReference, namespace string) (*controllerAndScale, error) {
 	ok, err := verifyGroupKind(controllerRef, controllerKindRC.Kind, []string{""})
 	if !ok || err != nil {
@@ -363,7 +389,7 @@
 	klog.Infof("Starting disruption controller")
 	defer klog.Infof("Shutting down disruption controller")

-	if !cache.WaitForNamedCacheSync("disruption", stopCh, dc.podListerSynced, dc.pdbListerSynced, dc.rcListerSynced, dc.rsListerSynced, dc.dListerSynced, dc.ssListerSynced) {
+	if !cache.WaitForNamedCacheSync("disruption", stopCh, dc.podListerSynced, dc.pdbListerSynced, dc.rcListerSynced, dc.rsListerSynced, dc.dListerSynced, dc.ssListerSynced, dc.dsListerSynced) {
 		return
 	}

Index: pkg/controller/disruption/disruption_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pkg/controller/disruption/disruption_test.go b/pkg/controller/disruption/disruption_test.go
--- a/pkg/controller/disruption/disruption_test.go	(revision 151a50e8e9872415562d9dc7b94081ef32fbef5f)
+++ b/pkg/controller/disruption/disruption_test.go	(revision 1d028b69c6385bc4bcd5a678aec3186c3fae7da4)
@@ -134,6 +134,7 @@
 	rsStore  cache.Store
 	dStore   cache.Store
 	ssStore  cache.Store
+	dsStore  cache.Store

 	coreClient      *fake.Clientset
 	scaleClient     *scalefake.FakeScaleClient
@@ -166,6 +167,7 @@
 		informerFactory.Apps().V1().ReplicaSets(),
 		informerFactory.Apps().V1().Deployments(),
 		informerFactory.Apps().V1().StatefulSets(),
+		informerFactory.Apps().V1().DaemonSets(),
 		coreClient,
 		testrestmapper.TestOnlyStaticRESTMapper(scheme),
 		fakeScaleClient,
@@ -178,6 +180,7 @@
 	dc.rsListerSynced = alwaysReady
 	dc.dListerSynced = alwaysReady
 	dc.ssListerSynced = alwaysReady
+	dc.dsListerSynced = alwaysReady

 	informerFactory.Start(context.TODO().Done())
 	informerFactory.WaitForCacheSync(nil)
@@ -190,6 +193,7 @@
 		informerFactory.Apps().V1().ReplicaSets().Informer().GetStore(),
 		informerFactory.Apps().V1().Deployments().Informer().GetStore(),
 		informerFactory.Apps().V1().StatefulSets().Informer().GetStore(),
+		informerFactory.Apps().V1().DaemonSets().Informer().GetStore(),
 		coreClient,
 		fakeScaleClient,
 		fakeDiscovery,
@@ -277,6 +281,13 @@
 	pod.OwnerReferences = append(pod.OwnerReferences, controllerReference)
 }

+func updatePodOwnerToDs(t *testing.T, pod *v1.Pod, ds *apps.DaemonSet) {
+	var controllerReference metav1.OwnerReference
+	var trueVar = true
+	controllerReference = metav1.OwnerReference{UID: ds.UID, APIVersion: controllerKindDS.GroupVersion().String(), Kind: controllerKindDS.Kind, Name: ds.Name, Controller: &trueVar}
+	pod.OwnerReferences = append(pod.OwnerReferences, controllerReference)
+}
+
 func newPod(t *testing.T, name string) (*v1.Pod, string) {
 	pod := &v1.Pod{
 		TypeMeta: metav1.TypeMeta{APIVersion: "v1"},
@@ -400,6 +411,32 @@
 	return ss, ssName
 }

+func newDaemonSet(t *testing.T, size int32) (*apps.DaemonSet, string) {
+	ds := &apps.DaemonSet{
+		TypeMeta: metav1.TypeMeta{APIVersion: "v1"},
+		ObjectMeta: metav1.ObjectMeta{
+			UID:             uuid.NewUUID(),
+			Name:            "foobar",
+			Namespace:       metav1.NamespaceDefault,
+			ResourceVersion: "18",
+			Labels:          fooBar(),
+		},
+		Spec: apps.DaemonSetSpec{
+			Selector: newSelFooBar(),
+		},
+		Status: apps.DaemonSetStatus{
+			DesiredNumberScheduled: size,
+		},
+	}
+
+	dsName, err := controller.KeyFunc(ds)
+	if err != nil {
+		t.Fatalf("Unexpected error naming DaemonSet %q: %v", ds.Name, err)
+	}
+
+	return ds, dsName
+}
+
 func update(t *testing.T, store cache.Store, obj interface{}) {
 	if err := store.Update(obj); err != nil {
 		t.Fatalf("Could not add %+v to %+v: %v", obj, store, err)
@@ -839,6 +876,37 @@
 	}
 }

+func TestDaemonSetController(t *testing.T) {
+	labels := map[string]string{
+		"foo": "bar",
+		"baz": "quux",
+	}
+
+	dc, ps := newFakeDisruptionController()
+
+	// 34% should round up to 2
+	pdb, pdbName := newMinAvailablePodDisruptionBudget(t, intstr.FromString("34%"))
+	add(t, dc.pdbStore, pdb)
+	ds, _ := newDaemonSet(t, 3)
+	add(t, dc.dsStore, ds)
+	dc.sync(pdbName)
+
+	ps.VerifyPdbStatus(t, pdbName, 0, 0, 0, 0, map[string]metav1.Time{})
+
+	for i := int32(0); i < 3; i++ {
+		pod, _ := newPod(t, fmt.Sprintf("foobar %d", i))
+		updatePodOwnerToDs(t, pod, ds)
+		pod.Labels = labels
+		add(t, dc.podStore, pod)
+		dc.sync(pdbName)
+		if i < 2 {
+			ps.VerifyPdbStatus(t, pdbName, 0, i+1, 2, 3, map[string]metav1.Time{})
+		} else {
+			ps.VerifyPdbStatus(t, pdbName, 1, 3, 2, 3, map[string]metav1.Time{})
+		}
+	}
+}
+
 func TestTwoControllers(t *testing.T) {
 	// Most of this test is in verifying intermediate cases as we define the
 	// three controllers and create the pods.
@@ -992,6 +1060,8 @@
 	add(t, dc.rcStore, rc)
 	ss, _ := newStatefulSet(t, 14)
 	add(t, dc.ssStore, ss)
+	ds, _ := newDaemonSet(t, 13)
+	add(t, dc.dsStore, ds)

 	testCases := map[string]struct {
 		finderFunc    podControllerFinder
@@ -1052,6 +1122,23 @@
 			name:       ss.Name,
 			uid:        ss.UID,
 			findsScale: false,
+		},
+		"daemonset controller with extensions group": {
+			finderFunc:    dc.getPodDaemonSet,
+			apiVersion:    "apps/v1",
+			kind:          controllerKindDS.Kind,
+			name:          ds.Name,
+			uid:           ds.UID,
+			findsScale:    true,
+			expectedScale: 13,
+		},
+		"daemonset controller with invalid kind": {
+			finderFunc: dc.getPodDaemonSet,
+			apiVersion: "apps/v1",
+			kind:       controllerKindRS.Kind,
+			name:       ds.Name,
+			uid:        ds.UID,
+			findsScale: false,
 		},
 	}

